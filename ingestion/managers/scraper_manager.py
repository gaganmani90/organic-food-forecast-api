import os

from ingestion.utils.data_saver import DataSaver


class ScraperManager:
    def __init__(self, scraper, output_filename):
        self.scraper = scraper
        self.output_json = os.path.join("output", f"{output_filename}.json")

    def scrape(self, max_pages=50, force_scrape=False):
        if not force_scrape and os.path.exists(self.output_json):
            print(f"âœ… Data already exists. Skipping scraping.")
            return

        print(f"ğŸ” Starting scraping for {self.scraper.state} up to {max_pages} pages...")
        all_data = self.scraper.extract_data(force_scrape, max_pages)

        print(f"âœ… Total records extracted: {len(all_data)}")
        DataSaver.save_to_json(all_data, self.output_json)

        # âœ… Call MetadataLogger after scraping
        # MetadataLogger.save_metadata()
